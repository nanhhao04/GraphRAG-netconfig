[
    {
        "file_name": "app.py",
        "file_path": "src\\app.py",
        "imports": [
            "import streamlit as st\nimport time\nimport os\nimport sys",
            "import src.connection as connection\nfrom src.graph import",
            "from src.retrieval import router_search, global_search,"
        ],
        "content": "import streamlit as st\nimport time\nimport os\nimport sys\n\n# ƒê·∫£m b·∫£o python t√¨m th·∫•y c√°c module trong th∆∞ m·ª•c src\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\n# Import c√°c module c·ªßa b·∫°n\nimport src.connection as connection\nfrom src.graph import (run_ingestion, run_clustering_louvain, run_summarization)\nfrom src.retrieval import router_search, global_search, \\\n    local_search  # L∆∞u √Ω: route_question hay router_search tu·ª≥ t√™n h√†m b·∫°n ƒë·∫∑t\n\nst.set_page_config(\n    page_title=\"Network GraphRAG AI\",\n    page_icon=\"üï∏Ô∏è\",\n    layout=\"wide\"\n)\n\n# CSS T√ôY CH·ªàNH\nst.markdown(\"\"\"\n<style>\n    /* Ch·ªânh m√†u n·ªÅn v√† border cho khung chat */\n    .stChatMessage {\n        border-radius: 15px;\n        padding: 10px;\n        margin-bottom: 10px;\n        border: 1px solid #e0e0e0;\n    }\n    /* L√†m ƒë·∫≠m c√°c th·∫ª Header trong Markdown */\n    h2, h3 {\n        color: #2E86C1; /* M√†u xanh chuy√™n nghi·ªáp */\n    }\n    /* Hi·ªáu ·ª©ng cho status box */\n    .stStatusWidget {\n        border-radius: 10px;\n    }\n</style>\n\"\"\", unsafe_allow_html=True)\n\n\n#H√ÄM X·ª¨ L√ù FILE UPLOAD\ndef process_uploaded_yaml(uploaded_file):\n    \"\"\"\n    H√†m ƒë·ªçc file upload t·ª´ Streamlit v·ªõi c∆° ch·∫ø b·∫Øt l·ªói an to√†n.\n    \"\"\"\n    if uploaded_file is None:\n        st.warning(\"Vui l√≤ng upload file YAML tr∆∞·ªõc khi x√¢y d·ª±ng Graph.\")\n        return None\n\n    try:\n        content = uploaded_file.read().decode(\"utf-8\")\n        if not content.strip():\n            st.error(\"L·ªói: File t·∫£i l√™n b·ªã r·ªóng!\")\n            return None\n        return content\n    except Exception as e:\n        st.error(f\"L·ªói khi ƒë·ªçc file: {e}\")\n        return None\n\n\n# INIT CONNECTION\n@st.cache_resource\ndef setup_connections():\n    try:\n        connection.init_connections()\n        return True\n    except Exception as e:\n        st.error(f\"L·ªói k·∫øt n·ªëi Neo4j/Gemini: {e}\")\n        return False\n\n\nif not setup_connections():\n    st.stop()\n\n\n# SIDEBAR\n\nwith st.sidebar:\n    st.title(\" Admin Control\")\n    st.markdown(\"---\")\n\n    st.subheader(\"1. Qu·∫£n l√Ω D·ªØ li·ªáu Graph\")\n    uploaded_file = st.file_uploader(\"Upload file YAML c·∫•u h√¨nh m·∫°ng\", type=[\"yml\", \"yaml\"])\n\n    if st.button(\"X√¢y d·ª±ng Graph (Full Flow)\", type=\"primary\"):\n        # G·ªçi h√†m x·ª≠ l√Ω file an to√†n\n        yaml_content = process_uploaded_yaml(uploaded_file)\n\n        # Ch·ªâ ch·∫°y ti·∫øp n·∫øu c√≥ n·ªôi dung\n        if yaml_content:\n            with st.status(\"ƒêang x√¢y d·ª±ng Knowledge Graph...\", expanded=True) as status:\n                st.write(\"1. Reading & Ingesting Data...\")\n                run_ingestion(yaml_content)\n\n                st.write(\"2. Running Louvain Clustering...\")\n                run_clustering_louvain()\n\n                status.update(label=\"X√¢y d·ª±ng Graph ho√†n t·∫•t!\", state=\"complete\", expanded=False)\n            st.success(\"H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!\")\n\n    st.markdown(\"---\")\n    st.subheader(\"2. Ch·∫ø ƒë·ªô T√¨m ki·∫øm\")\n    # ƒê√ÇY L√Ä CH·ªñ KHAI B√ÅO BI·∫æN search_mode\n    search_mode = st.radio(\n        \"Ch·ªçn ch·∫ø ƒë·ªô:\",\n        (\"Auto (AI Router)\", \"Global Search (T·ªïng quan)\", \"Local Search (Chi ti·∫øt)\")\n    )\n\n    st.markdown(\"---\")\n    if st.button(\"X√≥a l·ªãch s·ª≠ chat\"):\n        st.session_state.messages = []\n        st.rerun()\n\n\n# MAIN CHAT INTERFACE\nst.title(\"üï∏Ô∏è Network GraphRAG Assistant\")\nst.caption(\"Powered by Neo4j & Gemini 1.5 Flash | Graph-based Reasoning\")\n\n# 1. Kh·ªüi t·∫°o l·ªãch s·ª≠ chat\nif \"messages\" not in st.session_state:\n    st.session_state.messages = [\n        {\"role\": \"assistant\",\n         \"content\": \"Xin ch√†o! T√¥i l√† tr·ª£ l√Ω m·∫°ng AI. T√¥i ƒë√£ s·∫µn s√†ng ph√¢n t√≠ch h·ªá th·ªëng c·ªßa b·∫°n.\"}\n    ]\n\n# 2. Hi·ªÉn th·ªã l·ªãch s·ª≠ chat\nfor msg in st.session_state.messages:\n    avatar = \"ü§ñ\" if msg[\"role\"] == \"assistant\" else \"üßë‚Äçüíª\"\n    with st.chat_message(msg[\"role\"], avatar=avatar):\n        st.markdown(msg[\"content\"])\n\n# 3. X·ª≠ l√Ω Input\nif prompt := st.chat_input(\"VD: H·ªá th·ªëng c√≥ ƒëi·ªÉm ƒë∆°n th·∫•t b·∫°i (SPOF) n√†o kh√¥ng?\"):\n    # Hi·ªÉn th·ªã c√¢u h·ªèi User\n    with st.chat_message(\"user\", avatar=\"üßë‚Äçüíª\"):\n        st.markdown(prompt)\n    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n\n    # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi AI\n    with st.chat_message(\"assistant\", avatar=\"ü§ñ\"):\n        message_placeholder = st.empty()\n        full_response = \"\"\n\n        # D√πng st.status thay cho spinner\n        with st.status(\"ƒêang ph√¢n t√≠ch h·ªá th·ªëng...\", expanded=True) as status:\n            try:\n                # Logic ch·ªçn h√†m search (S·ª≠ d·ª•ng bi·∫øn search_mode t·ª´ sidebar)\n                if search_mode == \"Auto (AI Router)\":\n                    st.write(\"Targeting: AI Router Decision...\")\n                    # L∆∞u √Ω: Import ƒë√∫ng t√™n h√†m router c·ªßa b·∫°n (route_question ho·∫∑c router_search)\n                    response_text = router_search(prompt)\n                elif search_mode == \"Global Search (T·ªïng quan)\":\n                    st.write(\"Targeting: Global Map-Reduce Analysis...\")\n                    response_text = global_search(prompt)\n                else:\n                    st.write(\"Targeting: Local Entity Traversal...\")\n                    response_text = local_search(prompt)\n\n                status.update(label=\"Ph√¢n t√≠ch ho√†n t·∫•t!\", state=\"complete\", expanded=False)\n\n                # Hi·ªáu ·ª©ng g√µ ch·ªØ\n                for chunk in response_text.split(\" \"):\n                    full_response += chunk + \" \"\n                    time.sleep(0.01)\n                    message_placeholder.markdown(full_response + \"‚ñå\")\n\n                message_placeholder.markdown(full_response)\n\n            except Exception as e:\n                status.update(label=\"Err: C√≥ l·ªói x·∫£y ra!\", state=\"error\")\n                st.error(f\"Chi ti·∫øt l·ªói: {e}\")\n                full_response = \"Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi truy xu·∫•t d·ªØ li·ªáu.\"\n                message_placeholder.markdown(full_response)\n\n    st.session_state.messages.append({\"role\": \"assistant\", \"content\": full_response})"
    },
    {
        "file_name": "connection.py",
        "file_path": "src\\connection.py",
        "imports": [
            "import os\nimport yaml\nfrom langchain_community.graphs import Neo4jGraph\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom neo4j import GraphDatabase"
        ],
        "content": "import os\nimport yaml\nfrom langchain_community.graphs import Neo4jGraph\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom neo4j import GraphDatabase\n\n# --- 1. KHAI B√ÅO BI·∫æN GLOBAL (M·∫∑c ƒë·ªãnh l√† None) ---\ncfg = {}\ngraph = None\nllm = None\nembeddings = None\ndriver = None\n\n\ndef load_config():\n    current_dir = os.path.dirname(os.path.abspath(__file__))\n    config_path = os.path.join(current_dir, \"..\", \"config.yml\")\n\n    try:\n        with open(config_path, \"r\") as f:\n            return yaml.load(f, Loader=yaml.FullLoader)\n    except FileNotFoundError:\n        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file c·∫•u h√¨nh t·∫°i {config_path}\")\n        return {}\n\n\ndef init_connections():\n    global cfg, graph, llm, embeddings, driver\n    print(\"ƒêang k·∫øt n·ªëi neo4j v√† gemini ..........\")\n\n    # Load c·∫•u h√¨nh\n    cfg = load_config()\n    if not cfg:\n        print(\"C·∫£nh b√°o: Config r·ªóng!\")\n        return\n\n    #K·∫æT N·ªêI GEMINI\n    api_key = cfg.get(\"GOOGLE_API_KEY\")\n    if api_key:\n        try:\n            llm = ChatGoogleGenerativeAI(\n                model=\"gemini-2.5-flash\",\n                google_api_key=api_key,\n                temperature=0\n\n            )\n            embeddings = GoogleGenerativeAIEmbeddings(\n                model=\"models/text-embedding-004\",\n                google_api_key=api_key\n            )\n            print(\"Gemini Connected (LLM Ready)\")\n        except Exception as e:\n            print(f\"L·ªói k·∫øt n·ªëi Gemini: {e}\")\n            llm = None\n    else:\n        print(\"L·ªói: Thi·∫øu GOOGLE_API_KEY trong file config.yml\")\n\n    # K·∫æT N·ªêI NEO4J\n    uri = cfg.get(\"NEO4J_URI\")\n    user = cfg.get(\"NEO4J_USERNAME\")\n    pwd = cfg.get(\"NEO4J_PASSWORD\")\n\n    if uri and user and pwd:\n        try:\n            graph = Neo4jGraph(url=uri, username=user, password=pwd)\n            # Test k·∫øt n·ªëi\n            graph.query(\"RETURN 1\")\n            print(\"Neo4j Connected (Graph DB Ready)\")\n        except Exception as e:\n            print(f\"L·ªói k·∫øt n·ªëi Neo4j: {e}\")\n            graph = None\n    else:\n        print(\"C·∫£nh b√°o: Thi·∫øu th√¥ng tin k·∫øt n·ªëi Neo4j\")\n\n\nif __name__ == \"__main__\":\n    init_connections()\n    print(f\"Ki·ªÉm tra bi·∫øn LLM: {type(llm)}\")"
    },
    {
        "file_name": "graph.py",
        "file_path": "src\\graph.py",
        "imports": [
            "from langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_community.vectorstores import Neo4jVector\nimport json\nimport src.connection as connection\nfrom src.prompt.index.community_report_new import BATCH_COMMUNITY_REPORT_PROMPT\nfrom src.prompt.index.extract_graph import GRAPH_EXTRACTION_PROMPT\nfrom src.prompt.index.extract_graph_code_repo import GRAPH_EXTRACTION_REPO_PROMPT\nfrom src.prompt.index.community_report import COMMUNITY_REPORT_PROMPT\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom neo4j import GraphDatabase\n\nfrom langchain_core.output_parsers import StrOutputParser\nimport networkx as nx\nimport os",
            "import time\n    t1",
            "import time\n    t1",
            "import os\n    DATA_FILE_PATH"
        ],
        "content": "from langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_community.vectorstores import Neo4jVector\nimport json\nimport src.connection as connection\nfrom src.prompt.index.community_report_new import BATCH_COMMUNITY_REPORT_PROMPT\nfrom src.prompt.index.extract_graph import GRAPH_EXTRACTION_PROMPT\nfrom src.prompt.index.extract_graph_code_repo import GRAPH_EXTRACTION_REPO_PROMPT\nfrom src.prompt.index.community_report import COMMUNITY_REPORT_PROMPT\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom neo4j import GraphDatabase\n\nfrom langchain_core.output_parsers import StrOutputParser\nimport networkx as nx\nimport os\n\n\n\n#  1. INGESTION\ndef run_ingestion(yaml_content):\n    import time\n    t1 = time.time()\n    print(\"[1/3] Running Extraction...\")\n\n    prompt = PromptTemplate.from_template(GRAPH_EXTRACTION_PROMPT)\n    chain = prompt | connection.llm | StrOutputParser()\n\n\n    try:\n        result_text = chain.invoke({\n            \"input_text\": str(yaml_content),\n            \"entity_types\": \"DEVICE,INTERFACE,IP_ADDRESS,PROTOCOL\",\n            \"tuple_delimiter\": \"|\",\n            \"record_delimiter\": \"\\n\",\n            \"completion_delimiter\": \"<DONE>\"\n        })\n\n        with open(\"log/index/resultindex.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(result_text)\n\n    except Exception as e:\n        print(f\"Extraction Failed: {e}\")\n        return\n\n    entities = []\n    relationships = []\n\n    # T√°ch t·ª´ng d√≤ng\n    lines = result_text.strip().split(\"\\n\")\n\n    for line in lines:\n        line = line.strip()\n        if not line or line == \"<DONE>\": continue\n\n        if line.startswith(\"(\") and line.endswith(\")\"):\n            line = line[1:-1]\n\n        parts = line.split(\"|\")\n\n        # Parse Entity: \"entity\"|name|type|desc\n        if len(parts) >= 4 and \"entity\" in parts[0].lower():\n            entities.append({\n                \"name\": parts[1].strip(),\n                \"type\": parts[2].strip(),\n                \"desc\": parts[3].strip()\n            })\n\n        # Parse Relationship: \"relationship\"|src|tgt|desc|strength\n        elif len(parts) >= 5 and \"relationship\" in parts[0].lower():\n            relationships.append({\n                \"source\": parts[1].strip(),\n                \"target\": parts[2].strip(),\n                \"desc\": parts[3].strip(),\n                \"strength\": parts[4].strip()\n            })\n\n    print(f\"   -> Extracted {len(entities)} Entities & {len(relationships)} Relationships.\")\n    with open(\"log/index/EntityRelationship.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(entities, f, ensure_ascii=False, indent=2)\n        json.dump(relationships, f, ensure_ascii=False, indent=2)\n\n    # N·∫°p v√†o Neo4j\n    print(\"   -> Writing to Neo4j...\")\n    #connection.graph.query(\"MATCH (n) DETACH DELETE n\")  # Reset DB\n\n    # 1. T·∫°o Nodes\n    for ent in entities:\n        # Chu·∫©n h√≥a nh√£n\n        raw_type = ent['type'].upper().strip()\n        label = raw_type.replace(\" \", \"_\").replace(\"{\", \"\").replace(\"}\", \"\")\n\n\n        # G√°n nh√£n chung :Entity v√† nh√£n ri√™ng\n        query = f\"\"\"\n            MERGE (e:Entity {{id: $name}})\n            SET e:{label}, e.type = $type, e.desc = $desc\n        \"\"\"\n        connection.graph.query(query, {\n            \"name\": ent['name'],\n            \"type\": ent['type'],\n            \"desc\": ent['desc']\n        })\n\n    # 2. T·∫°o Edges\n    for rel in relationships:\n        connection.graph.query(\n            \"\"\"\n            MATCH (a:Entity {id: $src}), (b:Entity {id: $tgt})\n            MERGE (a)-[r:CONNECTED_TO]->(b)\n            SET r.desc = $desc, \n                r.strength = toInteger($strength)\n            \"\"\",\n            {\n                \"src\": rel['source'],\n                \"tgt\": rel['target'],\n                \"desc\": rel['desc'],\n                \"strength\": rel.get('strength', '1')\n            }\n        )\n\n    #run_leiden()\n    #run_clustering_louvain()\n    t2 = time.time()\n    print(f\"Th·ªùi gian extract entities v√† realtionship: {t2-t1} (s)\")\n\n\ndef run_clustering_louvain():\n    import time\n    t1 = time.time()\n    print(\"[2/3] Running Louvain Algorithm (Client-side)...\")\n\n    # 1. T·∫£i Graph t·ª´ Neo4j v·ªÅ Python\n    data = connection.graph.query(\"\"\"\n        MATCH (s:Entity)-[r:CONNECTED_TO]->(t:Entity)\n        RETURN s.id as source, t.id as target\n    \"\"\")\n\n    if not data:\n        print(\"Graph tr·ªëng, b·ªè qua b∆∞·ªõc ph√¢n c·ª•m.\")\n        return\n\n    # 2. D·ª±ng ƒë·ªì th·ªã NetworkX\n    G = nx.Graph()\n    for row in data:\n        G.add_edge(row['source'], row['target'])\n\n    print(f\"   -> Loaded Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")\n\n    # 3. Ch·∫°y Louvain\n    try:\n        # H√†m n√†y tr·∫£ v·ªÅ list c√°c set: [{node1, node2}, {node3, node4}...]\n        # resolution=1.0 l√† m·ª©c ƒë·ªô ti√™u chu·∫©n, tƒÉng l√™n ƒë·ªÉ c·ª•m nh·ªè h∆°n, gi·∫£m ƒëi ƒë·ªÉ c·ª•m to h∆°n\n        communities = nx.community.louvain_communities(G, resolution=0, seed=123)\n\n        print(f\"   -> Found {len(communities)} communities.\")\n        communities_list = [list(c) for c in communities]\n        with open(\"log/index/communities.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(json.dumps(communities_list, ensure_ascii=False, indent=2))\n\n        # 4. C·∫≠p nh·∫≠t communityId ng∆∞·ª£c l·∫°i v√†o Neo4j\n        print(\"   -> Updating Neo4j...\")\n\n        # Reset communityId c≈©\n        connection.graph.query(\"MATCH (e:Entity) REMOVE e.communityId\")\n\n        # Duy·ªát v√† update\n        batch_queries = []\n        for i, members in enumerate(communities):\n            cid = str(i)\n            # Chuy·ªÉn set th√†nh list ƒë·ªÉ g·ª≠i v√†o query\n            member_list = list(members)\n\n            # Update batch cho nhanh\n            connection.graph.query(\"\"\"\n                UNWIND $members as name\n                MATCH (e:Entity {id: name})\n                SET e.communityId = $cid\n            \"\"\", {\"members\": member_list, \"cid\": cid})\n\n        # Chuy·ªÉn sang b∆∞·ªõc t√≥m t·∫Øt\n        run_summarization()\n\n    except Exception as e:\n        print(f\"Louvain Error: {e}\")\n        print(\"Fallback: G√°n t·∫•t c·∫£ v√†o Community 0\")\n        connection.graph.query(\"MATCH (e:Entity) SET e.communityId = '0'\")\n        run_summarization()\n\n\n    t2 = time.time()\n    print(f\"Louvain + sumary time: {t2-t1} (s)\")\n\n\n\ndef run_summarization():\n    print(\"[3/3] Generating Community Reports (Batch Mode)...\")\n\n    cids_result = connection.graph.query(\n        \"MATCH (d:Entity) WHERE d.communityId IS NOT NULL RETURN distinct d.communityId as cid\")\n\n    # Chuy·ªÉn k·∫øt qu·∫£ th√†nh list c√°c ID th·ª±c t·∫ø\n    all_cids = [r['cid'] for r in cids_result if r['cid'] is not None]\n    print(f\"all_cid:\\n {all_cids}\")\n\n    if not all_cids:\n        print(\" -> Kh√¥ng t√¨m th·∫•y Community n√†o.\")\n        return\n\n    BATCH_SIZE = 4\n    chunks = [all_cids[i:i + BATCH_SIZE] for i in range(0, len(all_cids), BATCH_SIZE)]\n    print(f\"   -> T·ªïng {len(all_cids)} c·ª•m. Chia th√†nh {len(chunks)} ƒë·ª£t x·ª≠ l√Ω.\")\n\n    prompt = PromptTemplate.from_template(BATCH_COMMUNITY_REPORT_PROMPT)\n    chain = prompt | connection.llm | JsonOutputParser()\n    all_findings = []\n    full_reports_data = []\n\n    for i, chunk in enumerate(chunks):\n        print(f\"   -> Processing Batch {i + 1}/{len(chunks)} (IDs: {chunk})...\")\n\n        batch_context_text = \"\"\n        for cid in chunk:\n            members = connection.graph.query(\n                \"MATCH (d:Entity {communityId: $cid}) RETURN d.id, d.type, d.desc\",\n                {\"cid\": cid}\n            )\n            batch_context_text += f\"\\n--- COMMUNITY ID: {cid} ---\\n\"\n            member_text = \"\\n\".join(\n                [f\"- [{m.get('d.type', 'Device')}] {m['d.id']}: {m.get('d.desc', '')}\" for m in members]\n            )\n            batch_context_text += member_text + \"\\n\"\n\n        try:\n            reports_list = chain.invoke({\n                \"input_text\": batch_context_text\n            })\n            full_reports_data.extend(reports_list)\n\n            # Ki·ªÉm tra xem k·∫øt qu·∫£ c√≥ ph·∫£i l√† list\n            if isinstance(reports_list, dict):\n                reports_list = [reports_list]\n\n            for report in reports_list:\n                r_id = str(report.get('id'))\n\n                if r_id not in chunk:\n                    print(f\"Warning: LLM returned unknown ID {r_id}\")\n                    continue\n\n                connection.graph.query(\"\"\"\n                    MERGE (c:Community {id: $cid})\n                    SET c.title = $title, \n                        c.summary = $summary, \n                        c.rating = $rating,\n                        c.rating_explanation = $explanation,\n                        c.findings = $findings\n                    WITH c\n                    MATCH (d:Entity {communityId: $cid})\n                    MERGE (d)-[:IN_COMMUNITY]->(c)\n                \"\"\", {\n                    \"cid\": r_id,\n                    \"title\": report.get('title', f\"Cluster {r_id}\"),\n                    \"summary\": report.get('summary', ''),\n                    \"rating\": report.get('rating', 0),\n                    \"explanation\": report.get('rating_explanation', ''),\n                    \"findings\": json.dumps(report.get('findings', []))\n                })\n\n                print(f\"      -> Done Cluster {r_id}: {report.get('title')}\")\n\n                if report.get('findings'):\n                    all_findings.extend(report.get('findings'))\n\n        except Exception as e:\n            print(f\" Batch Error: {e}\")\n\n    os.makedirs(\"log\", exist_ok=True)\n    with open(\"log/index/reportsummary.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(full_reports_data, f, ensure_ascii=False, indent=2)\n    # 4. T·∫°o Index\n    create_indices()\n\n\ndef create_indices():\n    try:\n        Neo4jVector.from_existing_graph(\n            embedding=connection.embeddings,\n            url=connection.cfg[\"NEO4J_URI\"],\n            username=connection.cfg[\"NEO4J_USERNAME\"],\n            password=connection.cfg[\"NEO4J_PASSWORD\"],\n            index_name=\"entity_index\",\n            node_label=\"Entity\",\n            text_node_properties=[\"id\", \"desc\", \"type\", \"infor\"],\n            embedding_node_property=\"embedding\"\n        )\n        print(\"   -> Entity Index Created.\")\n    except Exception as e:\n        print(f\"Index Error: {e}\")\n\n    print(\"Build Complete!\")\n\n\n'''\ndef run_leiden():\n    print(\"[2/3] Running Leiden Algorithm...\")\n    try:\n        # 1. D·ªçn d·∫πp projection c≈© (n·∫øu c√≥)\n        try:\n            connection.graph.query(\"CALL gds.graph.drop('net_graph', false)\")\n        except Exception:\n            pass\n\n        # 2. T·∫°o In-Memory Graph (Project)\n        connection.graph.query(\"\"\"\n            CALL gds.graph.project(\n                'net_graph',\n                'Device',\n                {CONNECTED_TO: {orientation: 'UNDIRECTED'}}\n            )\n        \"\"\")\n\n        # 3. Ch·∫°y thu·∫≠t to√°n Leiden & Ghi communityId\n        res = connection.graph.query(\"\"\"\n            CALL gds.leiden.write(\n                'net_graph',\n                {writeProperty: 'communityId'}\n            ) YIELD communityCount\n        \"\"\")\n\n        count = res[0]['communityCount']\n        print(f\"   -> Found {count} communities.\")\n\n        # 4. Gi·∫£i ph√≥ng b·ªô nh·ªõ\n        connection.graph.query(\"CALL gds.graph.drop('net_graph', false)\")\n\n        # Chuy·ªÉn sang b∆∞·ªõc t√≥m t·∫Øt\n        run_summarization()\n\n    except Exception as e:\n        print(f\"Leiden Error: {e}\")\n        print(\"G·ª£i √Ω: H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ c√†i plugin 'Graph Data Science' (GDS) trong Neo4j.\")\n\n'''\n\n'''\nif __name__ == '__main__':\n    import os\n    DATA_FILE_PATH = os.path.join(\"../data/networkconfig.yml\")\n    init_connections()\n    yaml_content = load_yaml_data()\n\n    if yaml_content:\n        run_ingestion(yaml_content)\n        run_clustering_louvain()\n        '''\n\n\n\n"
    },
    {
        "file_name": "main.py",
        "file_path": "src\\main.py",
        "imports": [
            "import sys\nimport os\nfrom src.connection import init_connections\nfrom src.run_ingestion_rulebased import run_ingestion_test\nfrom src.graph import run_ingestion, run_clustering_louvain, run_ingestion\nfrom src.retrieval import global_search, local_search, router_search,  local_search_semantic\nfrom src.test.repo_struct import run_ingestion_for_repo_struct\nimport yaml\nfrom src.eval.eval_ragas import run_eval_pipeline\nimport json\n\nCODE_DATA_FILE_PATH"
        ],
        "content": "import sys\nimport os\nfrom src.connection import init_connections\nfrom src.run_ingestion_rulebased import run_ingestion_test\nfrom src.graph import run_ingestion, run_clustering_louvain, run_ingestion\nfrom src.retrieval import global_search, local_search, router_search,  local_search_semantic\nfrom src.test.repo_struct import run_ingestion_for_repo_struct\nimport yaml\nfrom src.eval.eval_ragas import run_eval_pipeline\nimport json\n\nCODE_DATA_FILE_PATH = os.path.join(\"../data/sample_structed_repo.json\")\nDATA_FILE_PATH = os.path.join(\"../data/networkconfig.yml\")\nIMPORT_ANALYSIS_DATA_FILE_PATH = os.path.join(\"../data/project_structure.json\")\nSTRUCTURED_DATA_FILE_PATH = os.path.join(\"../data/extracted_sample_format.json\")\n\ndef load_json_data(file_path):\n\n    if not os.path.exists(file_path):\n        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file t·∫°i {file_path}\")\n        return None\n\n    with open(file_path, 'r', encoding='utf-8-sig') as f:\n        try:\n            data = json.load(f)\n            return data\n        except json.JSONDecodeError as e:\n            f.seek(0)\n            content = f.read().strip()\n            if not content:\n                print(\"L·ªói: File JSON r·ªóng!\")\n                return None\n            return json.loads(content)\n\n\ndef load_yaml_data():\n    if not os.path.exists(DATA_FILE_PATH):\n        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i '{DATA_FILE_PATH}'\")\n        return None\n\n    try:\n        print(f\"ƒêang ƒë·ªçc file: {DATA_FILE_PATH}...\")\n        with open(DATA_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n            content = f.read()\n            if not content.strip():\n                print(\"C·∫£nh b√°o: File r·ªóng!\")\n                return None\n            return content\n    except Exception as e:\n        print(f\"L·ªói khi ƒë·ªçc file: {e}\")\n        return None\n\ndef load_yaml_data_dict():\n    if not os.path.exists(DATA_FILE_PATH):\n        print(f\"L·ªói: Kh√¥ng t√¨m th·∫•y file d·ªØ li·ªáu t·∫°i '{DATA_FILE_PATH}'\")\n        return None\n\n    try:\n        print(f\"ƒêang ƒë·ªçc file: {DATA_FILE_PATH}...\")\n        with open(DATA_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n            docs = list(yaml.safe_load_all(f))\n\n            if not docs:\n                print(\"C·∫£nh b√°o: File r·ªóng!\")\n                return None\n\n            return docs\n    except Exception as e:\n        print(f\"L·ªói khi ƒë·ªçc file: {e}\")\n        return None\n\n\ndef main():\n    print(\"GRAPH RAG NETWORK SYSTEM \")\n    init_connections()\n\n    while True:\n        print(\"\\n--------\")\n        print(\"1. X√¢y d·ª±ng Graph (Ingest -> Leiden -> Summarize)\")\n        print(\"   (Ngu·ªìn: data/networkconfig.yml)\")\n        print(\"2.X√¢y c·ªông ƒë·ªìng v√† summary\")\n        print(\"3.Global Search (H·ªèi t·ªïng quan h·ªá th·ªëng)\")\n        print(\"4.Local Search (H·ªèi chi ti·∫øt thi·∫øt b·ªã/L·ªói)\")\n        print(\"5.Router search\")\n        print(\"6.Ragas\")\n        print(\"7.Tho√°t\")\n        print(\"-------\")\n\n        choice = input(\"Ch·ªçn ch·ª©c nƒÉng (1-7): \").strip()\n\n        if choice == \"1\":\n            # ƒê·ªçc d·ªØ li·ªáu t·ª´ file\n            import_analysis_data = load_json_data(IMPORT_ANALYSIS_DATA_FILE_PATH)\n            repo_structure_data = load_json_data(STRUCTURED_DATA_FILE_PATH)\n            run_ingestion_for_repo_struct(repo_structure_data, import_analysis_data)\n            #yaml_content = load_yaml_data()\n\n\n            #if yaml_content:\n                #run_ingestion_for_repo_struct(repo_structure_data,import_analysis_data)\n\n                #run_ingestion_test(yaml_content)\n\n        elif choice == \"2\":\n            run_clustering_louvain()\n\n        elif choice == \"3\":\n            q = input(\"\\nNh·∫≠p c√¢u h·ªèi t·ªïng quan (VD: H·ªá th·ªëng c√≥ bao nhi√™u c·ª•m? T√¨nh tr·∫°ng chung th·∫ø n√†o?): \")\n            if q.strip():\n                print(\"\\nBot ƒëang suy nghƒ© (Global Strategy)...\")\n                response = global_search(q)\n                print(f\"\\nTR·∫¢ L·ªúI:\\n{response}\")\n\n        elif choice == \"4\":\n            q = input(\"\\nNh·∫≠p c√¢u h·ªèi chi ti·∫øt (VD: Router A k·∫øt n·ªëi v·ªõi ai? IP c·ªßa Switch B?): \")\n            if q.strip():\n                print(\"\\nBot ƒëang suy nghƒ© (Local Strategy)...\")\n                #response = local_search_semantic(q)\n                response = local_search(q)\n                print(f\"\\nTR·∫¢ L·ªúI:\\n{response}\")\n\n        elif choice == \"5\":\n            q = input(\"\\nNh·∫≠p c√¢u h·ªèi : \")\n            if q.strip():\n                print(\"\\nBot ƒëang suy nghƒ© (Local Strategy)...\")\n                response = router_search(q)\n                print(f\"\\nTR·∫¢ L·ªúI:\\n{response}\")\n\n\n        elif choice == \"6\":\n            q = input(\"\\nNh·∫≠p c√¢u h·ªèi c·∫ßn ƒë√°nh gi√°: \").strip()\n            if not q: continue\n            print(\"N·∫øu c√≥ c√¢u tr·∫£ l·ªùi m·∫´u (Ground Truth), Ragas s·∫Ω ch·∫•m th√™m:\")\n            print(\"Context Precision (ƒê·ªô ch√≠nh x√°c ng·ªØ c·∫£nh) Context Recall (ƒê·ªô bao ph·ªß ng·ªØ c·∫£nh)\")\n            ground_truth_input = input(\"Nh·∫≠p c√¢u tr·∫£ l·ªùi m·∫´u (Enter ƒë·ªÉ b·ªè qua): \").strip()\n\n            ground_truth = ground_truth_input if ground_truth_input else None\n\n            print(\"\\nBot ƒëang suy nghƒ© (Local Strategy)...\")\n            response = local_search(q)\n            print(f\"\\nTR·∫¢ L·ªúI:\\n{response}\")\n\n            print(\"\\n[Ragas] ƒêang chu·∫©n b·ªã d·ªØ li·ªáu ƒë√°nh gi√°...\")\n            context_file_path = \"log/query/final_context_local.json\"\n\n            try:\n                with open(context_file_path, \"r\", encoding=\"utf-8\") as f:\n                    full_text = f.read()\n\n                if full_text.strip():\n                    context_used = [full_text]\n                else:\n                    context_used = [\"Context r·ªóng\"]\n\n            except Exception as e:\n                print(f\"L·ªói ƒë·ªçc file context json: {e}\")\n                context_used = [\"Kh√¥ng t√¨m th·∫•y file log context\"]\n\n            run_eval_pipeline(q, response, context_used, ground_truth)\n\n        elif choice == \"7\":\n            print(\"Tho√°t!\")\n            sys.exit()\n\n        else:\n            print(\"L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá. Vui l√≤ng ch·ªçn l·∫°i.\")\n\n\nif __name__ == \"__main__\":\n    if not os.path.exists(\"data\"):\n        os.makedirs(\"data\")\n\n    main()"
    },
    {
        "file_name": "retrieval.py",
        "file_path": "src\\retrieval.py",
        "imports": [
            "import json\nimport random\nimport time\nfrom collections import defaultdict\n\nimport tiktoken\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser, StrOutputParser\nfrom langchain_community.vectorstores import Neo4jVector\nimport src.connection as connection\n\nfrom src.prompt.query.global_search_map_system_prompt import MAP_SYSTEM_PROMPT\nfrom src.prompt.query.global_search_reduce_system_prompt import REDUCE_SYSTEM_PROMPT\nfrom src.prompt.query.local_search_system_prompt import LOCAL_SEARCH_SYSTEM_PROMPT\nfrom src.prompt.query.router_search import ROUTER_SYSTEM_PROMPT\n\n\ndef count_tokens"
        ],
        "content": "import json\nimport random\nimport time\nfrom collections import defaultdict\n\nimport tiktoken\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser, StrOutputParser\nfrom langchain_community.vectorstores import Neo4jVector\nimport src.connection as connection\n\nfrom src.prompt.query.global_search_map_system_prompt import MAP_SYSTEM_PROMPT\nfrom src.prompt.query.global_search_reduce_system_prompt import REDUCE_SYSTEM_PROMPT\nfrom src.prompt.query.local_search_system_prompt import LOCAL_SEARCH_SYSTEM_PROMPT\nfrom src.prompt.query.router_search import ROUTER_SYSTEM_PROMPT\n\n\ndef count_tokens(text):\n    try:\n        encoding = tiktoken.get_encoding(\"cl100k_base\") # Encoding chu·∫©n c·ªßa GPT-4\n        return len(encoding.encode(text))\n    except:\n        # Fallback n·∫øu ch∆∞a c√†i tiktoken: ∆∞·ªõc l∆∞·ª£ng 1 token ~ 4 k√Ω t·ª±\n        return len(text) // 4\n\n\n\ndef global_search(question):\n    print(\"GLOBAL SEARCH MODE (Map-Reduce Strategy)\")\n    t1 = time.time()\n\n# MAP\n    try:\n        communities = connection.graph.query(\"\"\"\n            MATCH (c:Community) \n            RETURN c.id as id, c.title as title, c.summary as summary, c.rating as rating\n        \"\"\")\n    except Exception as e:\n        return f\"L·ªói truy v·∫•n Neo4j: {e}\"\n\n    if not communities:\n        return \"Ch∆∞a c√≥ d·ªØ li·ªáu Community. H√£y ch·∫°y Ingestion tr∆∞·ªõc.\"\n\n    random.shuffle(communities)  # X√°o tr·ªôn ng·∫´u nhi√™n\n\n    CHUNK_SIZE = 5\n    chunks = [communities[i:i + CHUNK_SIZE] for i in range(0, len(communities), CHUNK_SIZE)]\n    print(f\" ƒê√£ chia {len(communities)} communities th√†nh {len(chunks)} chunks ƒë·ªÉ x·ª≠ l√Ω.\")\n\n    map_chain = PromptTemplate.from_template(MAP_SYSTEM_PROMPT) | connection.llm | JsonOutputParser()\n    all_points = []\n    global_search_report = []\n\n    for i, chunk in enumerate(chunks):\n        chunk_context = \"\"\n        for c in chunk:\n            chunk_context += f\"\\n---\\nCommunity ID: {c['id']}\\nTitle: {c['title']}\\nSummary: {c['summary']}\\n\"\n\n        try:\n            # AI ƒë·ªçc chunk v√† tr·∫£ v·ªÅ JSON ch·ª©a c√°c points k√®m score\n            res = map_chain.invoke({\"question\": question,\n                                    \"context_data\": chunk_context,\n                                    \"response_type\": \"JSON list of points\",\n                                    \"max_length\": \"2000\"\n                                    })\n            global_search_report.append(res)\n\n            if res.get('points'):\n                for p in res['points']:\n                    # N·ªôi dung + ƒêi·ªÉm s·ªë\n                    all_points.append({\n                        \"description\": p.get('description', ''),\n                        \"score\": p.get('score', 0)\n                    })\n\n        except Exception as e:\n            print(f\"L·ªói x·ª≠ l√Ω Chunk {i}: {e}\")\n            continue\n\n    if not all_points:\n        return \"Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong h·ªá th·ªëng.\"\n\n    with open(\"log/query/globalsearch.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(global_search_report, f, ensure_ascii=False, indent=2)\n\n    all_points.sort(key=lambda x: x['score'], reverse=True)\n    top_points = all_points[:50]\n    with open(\"log/query/top_points.json\", \"w\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(top_points, ensure_ascii=False, indent=2))\n\n    formatted_report = \"\\n\".join([f\"- [Score: {p['score']}] {p['description']}\" for p in top_points])\n    with open(\"log/query/formatted_report_map.json\", \"w\", encoding=\"utf-8\") as f:\n        f.write(json.dumps(formatted_report, ensure_ascii=False, indent=2))\n\n    print(f\"   -> T·ªïng h·ª£p {len(top_points)}/{len(all_points)} th√¥ng tin quan tr·ªçng nh·∫•t (Top Scores).\")\n\n# REDUCE\n    reduce_chain = PromptTemplate.from_template(REDUCE_SYSTEM_PROMPT) | connection.llm | StrOutputParser()\n    final_answer = reduce_chain.invoke({\n        \"question\": question,\n        \"report_data\": formatted_report,\n        \"response_type\": \"General Text Analysis\",\n        \"max_length\": \"4000\"\n    })\n    t2 = time.time()\n    print(f\"Th·ªùi gian global search: {t2-t1} (s)\")\n\n    return final_answer\n\n\ndef local_search(question):\n    print(\"LOCAL SEARCH MODE (Top-K Nodes + Top-K Relations Strategy)...\")\n    t1 = time.time()\n\n    SEARCH_K = 5\n    TOP_RELATIONS = 10\n    HOP1_DECAY = 1.0\n    HOP2_DECAY = 0.5\n\n    # 1. VECTOR SEARCH\n    try:\n        vector_store = Neo4jVector.from_existing_index(\n            embedding=connection.embeddings,\n            url=connection.cfg[\"NEO4J_URI\"],\n            username=connection.cfg[\"NEO4J_USERNAME\"],\n            password=connection.cfg[\"NEO4J_PASSWORD\"],\n            index_name=\"entity_index\",\n            text_node_property=\"id\"\n        )\n        docs_with_score = vector_store.similarity_search_with_score(question, k=SEARCH_K)\n        with open(\"log/query/anchor_local.txt\", \"w\", encoding=\"utf-8\") as f:\n            for doc, score in docs_with_score:\n                f.write(f\"SCORE: {score}\\n\")\n                f.write(doc.page_content + \"\\n\")\n                f.write(str(doc.metadata) + \"\\n\")\n                f.write(\"-\" * 50 + \"\\n\")\n\n    except Exception as e:\n        return f\"L·ªói Vector Index: {e}\"\n\n    if not docs_with_score:\n        return \"Kh√¥ng t√¨m th·∫•y thi·∫øt b·ªã n√†o li√™n quan.\"\n\n    print(f\" -> T√¨m th·∫•y {len(docs_with_score)} Anchor Nodes.\")\n\n    # X·ª¨ L√ù ANCHOR INFO & TRAVERSAL\n    anchor_infos = []\n    all_relationships = []\n    processed_rels = set()\n\n    for doc, score in docs_with_score:\n        dev_id = doc.page_content.strip()\n        if dev_id == \"UNKNOWN\": continue\n\n        # a. L∆∞u th√¥ng tin Anchor\n        node_desc = doc.metadata.get('desc', 'No description')\n        node_type = doc.metadata.get('type', 'Entity')\n        anchor_text = f\"Node: {dev_id} (Type: {node_type}). Info: {node_desc}\"\n        anchor_infos.append(anchor_text)\n\n        # b. Traversal Query (2 Hops)\n        # Qu√©t Hop 1 v√† Hop 2 v√† Lo·∫°i 'IN_COMMUNITY'\n        # UNION ƒë·ªÉ g·ªôp 2 truy v·∫•n + b·ªè l·∫∑p\n        traversal_query = \"\"\"\n            // Hop 1\n            MATCH (src:Entity {id: $id})-[r1]-(n1:Entity)\n            WHERE type(r1) <> 'IN_COMMUNITY'\n            RETURN \n                src.id as src, src.type as src_type,\n                type(r1) as rel, r1.desc as rel_desc,\n                n1.id as tgt, n1.type as tgt_type, n1.desc as tgt_desc,\n                1 as hops\n\n            UNION\n\n            // Hop 2\n            MATCH (src:Entity {id: $id})-[r1]-(n1:Entity)-[r2]-(n2:Entity)\n            WHERE type(r1) <> 'IN_COMMUNITY' AND type(r2) <> 'IN_COMMUNITY'\n            RETURN \n                n1.id as src, n1.type as src_type,\n                type(r2) as rel, r2.desc as rel_desc,\n                n2.id as tgt, n2.type as tgt_type, n2.desc as tgt_desc,\n                2 as hops\n        \"\"\"\n\n        paths = connection.graph.query(traversal_query, {\"id\": dev_id})\n        with open(\"log/query/query_traversal_local.txt\", \"a\", encoding=\"utf-8\") as f:\n            header = f\"\\n{'=' * 20} Traversal for: {dev_id} (Score: {score:.4f}) {'=' * 20}\\n\"\n            f.write(header)\n\n        for p in paths:\n            # (Sorted tuple ƒë·ªÉ A-B v√† B-A l√† m·ªôt)\n            rel_key = tuple(sorted([p['src'], p['tgt']])) + (p['rel'],)\n\n            if rel_key in processed_rels:\n                continue\n            processed_rels.add(rel_key)\n\n            # T√≠nh ƒëi·ªÉm cho Relationship n√†y\n            decay = HOP1_DECAY if p['hops'] == 1 else HOP2_DECAY\n            rel_score = score * decay\n\n            rel_desc_str = f\" ({p['rel_desc']})\" if p['rel_desc'] else \"\"\n            tgt_desc_str = f\" ({p['tgt_desc']})\" if p['tgt_desc'] else \"\"\n\n            # Format: [Type] Source --[RELATION (desc)]--> [Type] Target (desc)\n            semantic_text = (\n                f\"[{p['src_type']}] {p['src']} \"\n                f\"--[{p['rel']}{rel_desc_str}]--> \"\n                f\"[{p['tgt_type']}] {p['tgt']}{tgt_desc_str}\"\n            )\n\n            all_relationships.append({\n                \"text\": semantic_text,\n                \"score\": rel_score,\n                \"hops\": p['hops']\n            })\n\n    # RANKING & PRUNING\n    all_relationships.sort(key=lambda x: x['score'], reverse=True)\n\n    top_relations = all_relationships[:TOP_RELATIONS]\n\n    print(f\"   -> Thu th·∫≠p {len(all_relationships)} k·∫øt n·ªëi. L·ªçc l·∫•y Top {len(top_relations)}.\")\n\n    # CONTEXT CONSTRUCTION\n    context_parts = []\n    context_parts.append(\"PRIMARY ANCHOR NODES (Top 5 Matches)\")\n    context_parts.extend(anchor_infos)\n\n    context_parts.append(f\"\\nTOP {len(top_relations)} RELEVANT CONNECTIONS\")\n    for rel in top_relations:\n        context_parts.append(f\"- {rel['text']}\")\n\n    final_context_text = \"\\n\".join(context_parts)\n\n    try:\n        with open(\"log/query/final_context_local.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump({\n                \"llm_context\": context_parts\n            }, f, ensure_ascii=False, indent=2)\n    except Exception as e:\n        print(f\"L·ªói ghi log context: {e}\")\n\n# LLM GENERATION\n    chain = PromptTemplate.from_template(LOCAL_SEARCH_SYSTEM_PROMPT) | connection.llm | StrOutputParser()\n    t2 = time.time()\n    print(f\"Th·ªùi gian local search: {t2 - t1:.2f}s\")\n\n    return chain.invoke({\n        \"question\": question,\n        \"context_data\": final_context_text\n    })\n\n\ndef local_search_semantic(question):\n    print(\"LOCAL SEARCH MODE (Semantic + Cleaning Strategy)...\")\n    t1 = time.time()\n\n    # C·∫•u h√¨nh\n    SEARCH_K = 5\n    GRAPH_LIMIT = 200\n\n    # 1. VECTOR SEARCH\n    try:\n        vector_store = Neo4jVector.from_existing_index(\n            embedding=connection.embeddings,\n            url=connection.cfg[\"NEO4J_URI\"],\n            username=connection.cfg[\"NEO4J_USERNAME\"],\n            password=connection.cfg[\"NEO4J_PASSWORD\"],\n            index_name=\"entity_index\",\n            text_node_property=\"id\"\n        )\n        docs_with_score = vector_store.similarity_search_with_score(question, k=SEARCH_K)\n        with open(\"log/query/anchor_local.txt\", \"w\", encoding=\"utf-8\") as f:\n            for doc, score in docs_with_score:\n                f.write(f\"SCORE: {score}\\n\")\n                f.write(doc.page_content + \"\\n\")\n                f.write(str(doc.metadata) + \"\\n\")\n                f.write(\"-\" * 50 + \"\\n\")\n\n    except Exception as e:\n        return f\"L·ªói Vector Index: {e}\"\n\n    if not docs_with_score: return \"Kh√¥ng t√¨m th·∫•y thi·∫øt b·ªã n√†o li√™n quan.\"\n\n    # 2. DATA FETCHING (2-HOPS)\n    traversal_query = \"\"\"\n        MATCH (anchor:Entity {id: $id})-[r1]-(n1:Entity)\n        WHERE type(r1) <> 'IN_COMMUNITY'\n        RETURN \n            anchor.id as src_id, anchor.type as src_type, anchor.desc as src_desc,\n            type(r1) as rel_type, r1.desc as rel_desc,\n            n1.id as tgt_id, n1.type as tgt_type, n1.desc as tgt_desc\n\n        UNION\n\n        MATCH (anchor:Entity {id: $id})-[r1]-(n1:Entity)-[r2]-(n2:Entity)\n        WHERE type(r1) <> 'IN_COMMUNITY' AND type(r2) <> 'IN_COMMUNITY'\n        RETURN \n            n1.id as src_id, n1.type as src_type, n1.desc as src_desc,\n            type(r2) as rel_type, r2.desc as rel_desc,\n            n2.id as tgt_id, n2.type as tgt_type, n2.desc as tgt_desc\n        LIMIT $limit\n    \"\"\"\n\n    # C·∫•u tr√∫c d·ªØ li·ªáu (S·ª≠ d·ª•ng Set cho IP ƒë·ªÉ t·ª± kh·ª≠ tr√πng l·∫∑p)\n    devices_map = defaultdict(lambda: {\n        \"desc\": \"\",\n        \"interfaces\": defaultdict(lambda: {\"ip\": set(), \"desc\": \"\"}),\n        \"routes\": set()\n    })\n\n    # Map ƒë·ªÉ trace ng∆∞·ª£c t·ª´ Interface v·ªÅ Device\n    interface_parent_map = {}\n    node_scores = {}\n\n    def clean_text(text):\n        if not text: return \"\"\n        return text.replace(\")**\", \"\").strip()\n\n    raw_rows = []\n\n    # Fetch Data\n    for doc, score in docs_with_score:\n        dev_id = doc.page_content.strip()\n        if dev_id == \"UNKNOWN\": continue\n\n        node_scores[dev_id] = max(node_scores.get(dev_id, 0), score)\n\n        try:\n            results = connection.graph.query(traversal_query, {\"id\": dev_id, \"limit\": GRAPH_LIMIT})\n            with open(\"log/query/query_traversal_local.txt\", \"a\", encoding=\"utf-8\") as f:\n                header = f\"\\n{'=' * 20} Traversal for: {dev_id} (Score: {score:.4f}) {'=' * 20}\\n\"\n                f.write(header)\n\n                if results:\n                    log_content = json.dumps(results, ensure_ascii=False, indent=2)\n                    f.write(log_content)\n                else:\n                    f.write(\"-> [WARN] No neighbors found in Graph (Empty Result).\\n\")\n\n                f.write(\"\\n\" + \"-\" * 60 + \"\\n\")\n\n            raw_rows.extend(results)\n            #print(f\"CHECK: raw_rows: {len(raw_rows)}\")\n        except:\n            continue\n\n    # 3. GOM NH√ìM & L√ÄM S·∫†CH\n\n    # PASS 1: X√¢y khung Device - Interface\n    for row in raw_rows:\n        src_id, src_type = row['src_id'], row['src_type']\n        tgt_id, tgt_type = row['tgt_id'], row['tgt_type']\n\n        # Logic: Ch·ªâ map Interface v√†o Device n·∫øu ID Interface ch·ª©a ID Device\n        # Ho·∫∑c ch·∫•p nh·∫≠n map l·ªèng l·∫ªo nh∆∞ng ph·∫£i c·∫©n th·∫≠n\n\n        if src_type == 'DEVICE' and tgt_type in ['INTERFACE', 'BOND', 'VLAN', 'BRIDGE']:\n            devices_map[src_id]['desc'] = clean_text(row['src_desc'])\n            devices_map[src_id]['interfaces'][tgt_id]['desc'] = clean_text(row['tgt_desc'])\n            interface_parent_map[tgt_id] = src_id  # Mapping Interface -> Device\n\n        elif tgt_type == 'DEVICE' and src_type in ['INTERFACE', 'BOND', 'VLAN', 'BRIDGE']:\n            devices_map[tgt_id]['desc'] = clean_text(row['tgt_desc'])\n            devices_map[tgt_id]['interfaces'][src_id]['desc'] = clean_text(row['src_desc'])\n            interface_parent_map[src_id] = tgt_id\n\n    # PASS 2: G·∫Øn IP v√† Route\n    for row in raw_rows:\n        src_id, src_type = row['src_id'], row['src_type']\n        tgt_id, tgt_type = row['tgt_id'], row['tgt_type']\n        rel = row.get('rel_desc') if row.get('rel_desc') else row['rel_type']\n\n        # G·∫Øn IP (D√πng Set ƒë·ªÉ add, t·ª± ƒë·ªông lo·∫°i tr√πng)\n        if src_type == 'IP_ADDRESS' and tgt_id in interface_parent_map:\n            devices_map[interface_parent_map[tgt_id]]['interfaces'][tgt_id]['ip'].add(clean_text(src_id))\n        elif tgt_type == 'IP_ADDRESS' and src_id in interface_parent_map:\n            devices_map[interface_parent_map[src_id]]['interfaces'][src_id]['ip'].add(clean_text(tgt_id))\n\n        # G·∫Øn Routes\n        if src_type == 'DEVICE' and 'ROUTE' in rel.upper():\n            devices_map[src_id]['routes'].add(f\"To {clean_text(tgt_id)} via {rel}\")\n        elif tgt_type == 'DEVICE' and 'ROUTE' in rel.upper():\n            devices_map[tgt_id]['routes'].add(f\"To {clean_text(src_id)} via {rel}\")\n\n    # 4. RENDER TEXT\n    context_lines = []\n    sorted_devs = sorted(devices_map.keys(), key=lambda k: node_scores.get(k, 0), reverse=True)\n\n\n    context_lines.append(\"=== DEVICE CONFIGURATIONS ===\")\n\n    for dev_id in sorted_devs:\n        data = devices_map[dev_id]\n        if not data['interfaces'] and not data['routes']: continue\n\n        context_lines.append(f\"### DEVICE: {dev_id}\")\n\n        short_desc = data['desc'].split(\"Configuration includes\")[0].strip()\n        context_lines.append(f\"  Role: {short_desc}\")\n\n        if data['interfaces']:\n            context_lines.append(f\"  INTERFACES:\")\n            for iface in sorted(data['interfaces'].keys()):\n                iface_data = data['interfaces'][iface]\n\n                # Convert set IP back to list & sort\n                ips = sorted(list(iface_data['ip']))\n                ip_str = f\" [IPs: {', '.join(ips)}]\" if ips else \"\"\n\n                # Extract Attributes ng·∫Øn g·ªçn\n                attrs = []\n                desc_lower = iface_data['desc'].lower()\n                if 'mtu' in desc_lower: attrs.append(\"MTU:9000\")\n                if 'bond' in desc_lower: attrs.append(\"Type:Bond\")\n                attr_str = f\" ({', '.join(attrs)})\" if attrs else \"\"\n\n                context_lines.append(f\"    - {iface}{ip_str}{attr_str}\")\n\n        if data['routes']:\n            context_lines.append(f\"  ROUTING TABLE:\")\n            for r in sorted(list(data['routes'])):\n                context_lines.append(f\"    - {r}\")\n\n        context_lines.append(\"\")\n\n    final_context_str = \"\\n\".join(context_lines)\n\n    # Logging\n    try:\n        with open(\"log/query/final_context_local.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump({\"llm_context\": context_lines}, f, ensure_ascii=False, indent=2)\n    except:\n        pass\n\n    # LLM\n    chain = PromptTemplate.from_template(LOCAL_SEARCH_SYSTEM_PROMPT) | connection.llm | StrOutputParser()\n    t2 = time.time()\n    print(f\"Th·ªùi gian: {t2 - t1:.2f}s\")\n\n    return chain.invoke({\n        \"question\": question,\n        \"context_data\": final_context_str\n    })\n\n\n\ndef router_search(question):\n    try:\n        router_chain = PromptTemplate.from_template(ROUTER_SYSTEM_PROMPT) | connection.llm | JsonOutputParser()\n        decision = router_chain.invoke({\"question\": question})\n        destination = decision.get(\"destination\", \"LOCAL\").upper()\n\n        print(f\"   -> Decision: {destination} STRATEGY\")\n\n\n        if destination == \"GLOBAL\":\n            return global_search(question)\n        else:\n            return local_search(question)\n\n    except Exception as e:\n        print(f\"Router Error: {e}. Fallback to Local Search.\")\n        return local_search(question)\n"
    },
    {
        "file_name": "run_ingestion_rulebased.py",
        "file_path": "src\\run_ingestion_rulebased.py",
        "imports": [
            "import yaml\nimport json\nimport re\nimport os\nimport unicodedata\nimport src.connection as connection\n\n\nOUTPUT_JSON",
            "import traceback\n        traceback.print_exc"
        ],
        "content": "import yaml\nimport json\nimport re\nimport os\nimport unicodedata\nimport src.connection as connection\n\n\nOUTPUT_JSON = \"log/graph_output_test.json\"\nSKIP_KEYS = {'network', 'ethernets', 'bonds', 'vlans', 'bridges', 'version', 'renderer'}\nKEY_MAP = {\n    \"mtu\": \"MTU size\", \"addresses\": \"assigned IPs\",\n    \"gateway4\": \"gateway\", \"dhcp4\": \"DHCP status\",\n    \"id\": \"VLAN ID\", \"link\": \"uplink bond\",\n    \"mode\": \"mode\", \"lacp-rate\": \"LACP rate\",\n    \"to\": \"destination\", \"via\": \"next-hop\",\n    \"metric\": \"metric\", \"interfaces\": \"member interfaces\",\n    \"parameters\": \"parameters\", \"transmit-hash-policy\": \"hash policy\",\n    \"mii-monitor-interval\": \"monitor interval\"\n}\n\n# Globals\nentities = []\nrelationships = []\nnode_ids = set()\n\ndef remove_accents(input_str):\n    if not input_str: return \"\"\n    nfkd_form = unicodedata.normalize('NFKD', str(input_str))\n    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n\n\ndef clean_id(raw):\n    if not raw: return \"UNKNOWN\"\n\n    raw = remove_accents(str(raw)) # b·ªè d·∫•u ti·∫øng vi·ªát\n    raw = raw.upper().strip()\n    raw = re.sub(r'[^\\w\\d]', '_', raw)\n    raw = re.sub(r'_+', '_', raw)\n    return raw.strip('_')\n\n\ndef extract_device_names_from_raw(raw_text):\n    blocks = re.split(r'\\n---\\s*\\n', raw_text)\n    names = []\n\n    for idx, block in enumerate(blocks):\n        name = None\n        for line in block.strip().splitlines():\n            line = line.strip()\n            if line.startswith(\"#\"):\n                found_raw_name = None\n\n                m = re.search(r'DEVICE\\s*:\\s*(.+)', line, re.IGNORECASE)\n                if m:\n                    found_raw_name = m.group(1).strip()\n\n                else:\n                    content = line.lstrip(\"#\").strip()\n                    if content and \"CONFIG\" not in content.upper():\n                        found_raw_name = content\n\n                if found_raw_name:\n                    name = re.sub(r'\\s*\\(.*?\\)', '', found_raw_name).strip()\n                    break\n\n            if line and not line.startswith(\"#\"):\n                break  # D·ª´ng n·∫øu g·∫∑p content kh√¥ng ph·∫£i comment\n\n        names.append(name if name else f\"DEVICE_{idx + 1}\")\n\n    return names\n\ndef format_list_items(key, value_list):\n    if not value_list: return \"\"\n    try:\n        if isinstance(value_list[0], str):\n            return f\"{KEY_MAP.get(key, key)} [{', '.join(value_list)}]\"\n        if isinstance(value_list[0], dict):\n            items_desc = []\n            for item in value_list:\n                if 'to' in item and 'via' in item:\n                    route_str = f\"route to {item['to']} via {item['via']}\"\n                    if 'metric' in item: route_str += f\" (metric {item['metric']})\"\n                    items_desc.append(route_str)\n                else:\n                    items_desc.append(generate_semantic_desc(item))\n            return f\"Routes: {'; '.join(items_desc)}\"\n    except:\n        pass\n    return f\"{key}: {str(value_list)}\"\n\n\ndef generate_semantic_desc(data):\n    if isinstance(data, dict):\n        sentences = []\n        primitives = {k: v for k, v in data.items() if not isinstance(v, (dict, list))}\n        complex_data = {k: v for k, v in data.items() if isinstance(v, (dict, list))}\n\n        props_str = []\n        for k, v in primitives.items():\n            if k in ['renderer', 'version', 'optional']: continue\n            human_key = KEY_MAP.get(k, k)\n            props_str.append(f\"{human_key} {v}\")\n        if props_str: sentences.append(\", \".join(props_str) + \".\")\n\n        for k, v in complex_data.items():\n            if isinstance(v, list):\n                list_desc = format_list_items(k, v)\n                if list_desc: sentences.append(list_desc)\n            elif isinstance(v, dict):\n                child_desc = generate_semantic_desc(v)\n                if child_desc: sentences.append(f\"Section '{k}': [{child_desc}]\")\n\n        return \" \".join([s for s in sentences if s])\n    elif isinstance(data, list):\n        return format_list_items(\"items\", data)\n    return str(data)\n\n\ndef add_entity(raw_id, etype, info=None):\n    cid = clean_id(raw_id)\n    # T·∫°o m√¥ t·∫£ (Desc) t·ª´ info\n    semantic_desc = generate_semantic_desc(info) if info else f\"{etype} {cid}\"\n\n    # T·∫°o JSON info\n    try:\n        infor_str = json.dumps(info, ensure_ascii=False)\n    except:\n        infor_str = str(info)\n\n    if cid not in node_ids:\n        entities.append({\n            \"name\": cid,\n            \"type\": etype,\n            \"desc\": semantic_desc,\n            \"infor\": infor_str\n        })\n        node_ids.add(cid)\n    return cid\n\ndef add_relation(src, tgt, rel_type):\n    s = clean_id(src)\n    t = clean_id(tgt)\n    if s != t:\n        relationships.append({\n            \"source\": s,\n            \"target\": t,\n            \"rel_type\": rel_type,\n            \"strength\": 10\n        })\n\ndef walk(key, value, parent_id, root_device_id):\n    # DICT (Sections)\n    if isinstance(value, dict):\n        node_type = \"SECTION\"\n        k_lower = key.lower()\n\n        # X√°c ƒë·ªãnh Type\n        if any(x in k_lower for x in [\"eth\", \"eno\", \"wan\", \"lan\"]):\n            node_type = \"INTERFACE\"\n        elif \"bond\" in k_lower:\n            node_type = \"BOND\"\n        elif \"vlan\" in k_lower:\n            node_type = \"VLAN\"\n        elif \"bridge\" in k_lower:\n            node_type = \"BRIDGE\"\n\n        # N·∫øu l√† key c·∫•u tr√∫c (ethernets...), ch·ªâ duy·ªát con\n        if k_lower in SKIP_KEYS:\n            for ck, cv in value.items():\n                walk(ck, cv, parent_id, root_device_id)\n            return\n\n        # T·∫°o Node\n        # ID = Root + Key (VD: DEVICE_1_ETH0)\n        current_node_id = f\"{root_device_id}_{key}\"\n\n        # {key: value} v√†o info\n        nid = add_entity(current_node_id, node_type, info={key: value})\n        add_relation(parent_id, nid, \"CONTAINS\")\n\n        # ƒê·ªá quy xu·ªëng con\n        for ck, cv in value.items():\n            walk(ck, cv, nid, root_device_id)\n\n    # LIST (IPs, Routes)\n    elif isinstance(value, list):\n        for item in value:\n            # IP Address\n            if key == \"addresses\" and isinstance(item, str):\n                ip_id = add_entity(item, \"IP_ADDRESS\", info={\"address\": item})\n                add_relation(parent_id, ip_id, \"HAS_IP\")\n\n            # Routes\n            elif key == \"routes\" and isinstance(item, dict):\n                dst = item.get(\"to\")\n                via = item.get(\"via\")\n\n                if dst:\n                    dst_id = add_entity(dst, \"IP_NETWORK\", info=item)\n                    add_relation(root_device_id, dst_id, \"ROUTES_TO\")  # N·ªëi t·ª´ Root Device\n\n                if via:\n                    via_id = add_entity(via, \"IP_ADDRESS\", info={\"gateway\": via})\n                    add_relation(root_device_id, via_id, \"NEXT_HOP\")  # N·ªëi t·ª´ Root Device\n\n\ndef run_ingestion_test(yaml_content):\n    print(\"[Ingestion Refined] Starting (Based on Reference Code)...\")\n\n    # Reset globals\n    entities.clear()\n    relationships.clear()\n    node_ids.clear()\n\n    try:\n        raw_text = str(yaml_content)\n\n        # 1. L·∫•y t√™n t·ª´ comment (Logic tham chi·∫øu)\n        device_names = extract_device_names_from_raw(raw_text)\n        print(f\"   -> Detected Names: {device_names}\")\n\n        # 2. Parse YAML\n        docs = list(yaml.safe_load_all(raw_text))\n\n        device_idx = 0\n        for doc in docs:\n            if not doc or \"network\" not in doc:\n                continue\n\n            # L·∫•y t√™n t∆∞∆°ng ·ª©ng\n            dev_name_raw = device_names[device_idx] if device_idx < len(device_names) else f\"DEVICE_{device_idx + 1}\"\n            device_idx += 1\n\n            # T·∫°o Root Node (Device)\n            root_id = add_entity(dev_name_raw, \"DEVICE\", info=doc[\"network\"])\n            print(f\"   -> Processing: {dev_name_raw} ==> ID: {root_id}\")\n\n            # B·∫Øt ƒë·∫ßu duy·ªát ƒë·ªá quy (Walk)\n            for k, v in doc[\"network\"].items():\n                walk(k, v, root_id, root_id)\n\n        # 3. Save JSON Log\n        os.makedirs(\"log\", exist_ok=True)\n        with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n            json.dump({\n                \"entities\": entities,\n                \"relationships\": relationships\n            }, f, indent=2, ensure_ascii=False)\n\n        print(f\"   -> Extracted {len(entities)} Entities & {len(relationships)} Relationships.\")\n        print(f\"   -> Log saved: {OUTPUT_JSON}\")\n\n        # 4. Write to Neo4j (Ph·∫ßn n√†y ph·∫£i gi·ªØ l·∫°i ƒë·ªÉ h·ªá th·ªëng ch·∫°y ƒë∆∞·ª£c)\n        print(\"   -> Writing to Neo4j...\")\n        connection.graph.query(\"MATCH (n) DETACH DELETE n\")\n\n        if entities:\n            connection.graph.query(\"\"\"\n                UNWIND $data AS row\n                MERGE (e:Entity {id: row.name})\n                SET e.type = row.type, \n                    e.desc = row.desc,\n                    e.infor = row.infor\n            \"\"\", {\"data\": entities})\n\n        if relationships:\n            # Batch write edges\n            batch_size = 1000\n            for i in range(0, len(relationships), batch_size):\n                batch = relationships[i:i + batch_size]\n                connection.graph.query(\"\"\"\n                    UNWIND $data AS row\n                    MATCH (a:Entity {id: row.source})\n                    MATCH (b:Entity {id: row.target})\n                    MERGE (a)-[r:CONNECTED_TO]->(b)\n                    SET r.rel_type = row.rel_type,\n                        r.strength = row.strength,\n                        r.desc = row.rel_type\n                \"\"\", {\"data\": batch})\n\n        print(\"   -> Ingestion Complete!\")\n\n    except Exception as e:\n        print(f\"Critical Error: {e}\")\n        import traceback\n        traceback.print_exc()\n\n"
    },
    {
        "file_name": "eval_ragas.py",
        "file_path": "src\\eval\\eval_ragas.py",
        "imports": [
            "import os\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.run_config import RunConfig\nfrom ragas.metrics import",
            "from ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nimport src.connection as connection\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing import Any, List, Optional\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.messages import BaseMessage\n\n\n\nEVAL_LOG_DIR"
        ],
        "content": "import os\nimport json\nimport time\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nfrom datasets import Dataset\nfrom ragas import evaluate\nfrom ragas.run_config import RunConfig\nfrom ragas.metrics import (\n    faithfulness,\n    answer_relevancy,\n    context_precision,\n    context_recall,\n    answer_correctness\n)\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nimport src.connection as connection\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom typing import Any, List, Optional\nfrom langchain_core.callbacks import CallbackManagerForLLMRun\nfrom langchain_core.messages import BaseMessage\n\n\n\nEVAL_LOG_DIR = \"log/eval\"\nos.makedirs(EVAL_LOG_DIR, exist_ok=True)\n\n\nclass GeminiNoTemp(ChatGoogleGenerativeAI):\n    def _generate(\n            self,\n            messages: List[BaseMessage],\n            stop: Optional[List[str]] = None,\n            run_manager: Optional[CallbackManagerForLLMRun] = None,\n            **kwargs: Any,\n    ):\n        if \"temperature\" in kwargs:\n            del kwargs[\"temperature\"]\n        return super()._generate(messages, stop=stop, run_manager=run_manager, **kwargs)\n\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super(NumpyEncoder, self).default(obj)\n\n\nclass NetworkRagasEvaluator:\n    def __init__(self):\n        try:\n            api_key = connection.cfg.get(\"GOOGLE_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n\n            llm = GeminiNoTemp(\n                model=\"gemini-2.5-flash\",\n                google_api_key=api_key,\n            )\n            self.ragas_llm = LangchainLLMWrapper(llm)\n\n        except Exception as e:\n            print(f\"[Warn] L·ªói kh·ªüi t·∫°o GeminiNoTemp: {e}\")\n            self.ragas_llm = LangchainLLMWrapper(connection.llm)\n\n        self.ragas_embeddings = LangchainEmbeddingsWrapper(connection.embeddings)\n\n        self.metrics_no_ref = [\n            faithfulness,\n            answer_relevancy\n        ]\n\n        self.metrics_with_ref = [\n            context_precision,\n            context_recall,\n            answer_correctness\n        ]\n\n    def create_dataset(self, questions, answers, contexts, ground_truths=None):\n        data = {\n            \"question\": questions,\n            \"answer\": answers,\n            \"contexts\": contexts,\n        }\n        if ground_truths:\n            data[\"ground_truth\"] = ground_truths\n\n        return Dataset.from_dict(data)\n\n    def evaluate_single_turn(self, question, answer, retrieved_context, ground_truth=None):\n        print(f\"Evaluating: '{question}'...\")\n        start_time = time.time()\n\n        if isinstance(retrieved_context, str):\n            contexts = [[retrieved_context]]\n        elif isinstance(retrieved_context, list):\n            contexts = [retrieved_context]\n        else:\n            contexts = [[\"\"]]\n\n        ground_truths = [ground_truth] if ground_truth else None\n\n        dataset = self.create_dataset(\n            questions=[question],\n            answers=[answer],\n            contexts=contexts,\n            ground_truths=ground_truths\n        )\n\n        active_metrics = self.metrics_no_ref.copy()\n        if ground_truth:\n            active_metrics.extend(self.metrics_with_ref)\n\n        run_config = RunConfig(\n            max_workers=1,\n            timeout=120,\n            max_retries=3,\n            max_wait=60\n        )\n\n        try:\n            results = evaluate(\n                dataset=dataset,\n                metrics=active_metrics,\n                llm=self.ragas_llm,\n                embeddings=self.ragas_embeddings,\n                run_config=run_config\n            )\n\n            end_time = time.time()  # K·∫øt th√∫c b·∫•m gi·ªù\n            duration = end_time - start_time\n\n            self.save_results(results, duration)\n            return results\n\n        except Exception as e:\n            print(f\"Ragas Error: {e}\")\n            return None\n\n    def save_results(self, results, duration):\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        filename = f\"{EVAL_LOG_DIR}/ragas_report_{timestamp}.json\"\n\n        try:\n            if hasattr(results, 'to_pandas'):\n                res_dict = results.to_pandas().to_dict(orient=\"records\")[0]\n            else:\n                res_dict = dict(results)\n        except:\n            res_dict = {\"raw_result\": str(results)}\n\n        print(\"\\n\" + \"=\" * 50)\n        print(f\"EVALUATION REPORT (Time: {duration:.2f}s)\")\n        print(\"=\" * 50)\n\n        scores = {}\n        meta_info = {}\n\n        # Th√™m th·ªùi gian ch·∫°y v√†o log\n        scores[\"execution_time_seconds\"] = duration\n\n        for k, v in res_dict.items():\n            if k in ['question', 'answer', 'contexts', 'ground_truth', 'user_input', 'response']:\n                meta_info[k] = v\n                continue\n\n            try:\n                val = float(v)\n                scores[k] = val\n                print(f\"- {k:<25}: {val:.4f}\")\n            except (ValueError, TypeError):\n                meta_info[k] = v\n\n        print(f\"- {'execution_time':<25}: {duration:.2f}s\")\n        print(\"-\" * 50)\n\n        try:\n            with open(filename, \"w\", encoding=\"utf-8\") as f:\n                full_log = {**scores, **meta_info}\n                json.dump(full_log, f, ensure_ascii=False, indent=2 , cls=NumpyEncoder)\n            print(f\"Saved: {filename}\")\n        except Exception as e:\n            print(f\"Save Error: {e}\")\n\n        print(\"=\" * 50 + \"\\n\")\n\n\ndef run_eval_pipeline(question, answer, context_list, ground_truth=None):\n    evaluator = NetworkRagasEvaluator()\n    return evaluator.evaluate_single_turn(question, answer, context_list, ground_truth)"
    },
    {
        "file_name": "extract_relationship_tool.py",
        "file_path": "src\\test\\extract_relationship_tool.py",
        "imports": [
            "import os\nimport re\nimport json\n\n\ndef scan_project_imports"
        ],
        "content": "import os\nimport re\nimport json\n\n\ndef scan_project_imports(project_path, output_json, exclude_dirs=None, exclude_files=None):\n    \"\"\"\n    Duy·ªát to√†n b·ªô file trong project v√† tr√≠ch xu·∫•t th√¥ng tin import.\n\n    :param project_path: ƒê∆∞·ªùng d·∫´n t·ªõi th∆∞ m·ª•c project.\n    :param output_json: T√™n file JSON k·∫øt qu·∫£.\n    :param exclude_dirs: Danh s√°ch c√°c th∆∞ m·ª•c c·∫ßn b·ªè qua (v√≠ d·ª•: ['venv', '.git', '__pycache__']).\n    :param exclude_files: Danh s√°ch c√°c file c·∫ßn b·ªè qua (v√≠ d·ª•: ['setup.py']).\n    \"\"\"\n    if exclude_dirs is None:\n        exclude_dirs = {'.git', 'venv', '.venv', '__pycache__', 'node_modules', '.idea', '.vscode'}\n    if exclude_files is None:\n        exclude_files = set()\n\n    import_regex = re.compile(\n        r'^\\s*(?:import\\s+[\\w\\.,\\s]+|from\\s+[\\w\\.]+\\s+import\\s+[\\w\\.,\\s\\(\\)\\*]+)',\n        re.MULTILINE\n    )\n\n    project_data = []\n    code_content = []\n\n    for root, dirs, files in os.walk(project_path):\n        # Lo·∫°i b·ªè c√°c th∆∞ m·ª•c n·∫±m trong danh s√°ch ngo·∫°i l·ªá\n        dirs[:] = [d for d in dirs if d not in exclude_dirs]\n\n        for file in files:\n            if file in exclude_files:\n                continue\n\n            # Ch·ªâ qu√©t c√°c file m√£ ngu·ªìn\n            if file.endswith(('.py', '.js', '.ts', '.java')):\n                file_path = os.path.join(root, file)\n\n                try:\n                    with open(file_path, 'r', encoding='utf-8') as f:\n                        content = f.read()\n                    code_content.append(content)\n\n                    imports = import_regex.findall(content)\n                    imports = [imp.strip() for imp in imports]\n\n                    if imports:\n                        project_data.append({\n                            \"file_name\": file,\n                            \"file_path\": os.path.relpath(file_path, project_path),\n                            \"imports\": imports,\n                            \"content\": content,\n                        })\n                except Exception as e:\n                    print(f\"Kh√¥ng th·ªÉ ƒë·ªçc file {file_path}: {e}\")\n\n    # L∆∞u v√†o file JSON\n    try:\n        with open(output_json, 'w', encoding='utf-8') as jf:\n            json.dump(project_data, jf, ensure_ascii=False, indent=4)\n        print(f\"ƒê√£ l∆∞u th√¥ng tin v√†o {output_json}\")\n    except Exception as e:\n        print(f\"L·ªói khi l∆∞u file JSON: {e}\")\n\n\nif __name__ == \"__main__\":\n    # ƒê∆∞·ªùng d·∫´n project hi·ªán t·∫°i\n    #current_prj = os.getcwd()\n    current_file = os.path.abspath(__file__)\n    root = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))\n\n    scan_project_imports(\n        project_path=root,\n        output_json=\"../../data/project_structure.json\",\n        exclude_dirs={'.git', 'venv', 'log'},\n        exclude_files={'__init__.py'}\n    )"
    },
    {
        "file_name": "extract_repo_structure.py",
        "file_path": "src\\test\\extract_repo_structure.py",
        "imports": [
            "import os\nimport json\n\n\ndef extract_project_to_match_format"
        ],
        "content": "import os\nimport json\n\n\ndef extract_project_to_match_format(project_root, repo_name=\"web-application\", branch_name=\"main\"):\n\n    # 1. Th√™m 'log' v√†o danh s√°ch lo·∫°i b·ªè th∆∞ m·ª•c\n    exclude_dirs = {'.git', 'venv', '__pycache__', 'node_modules', '.vscode', '.idea', 'dist', 'build', 'log'}\n\n    # 2. Th√™m c√°c ƒëu√¥i file mu·ªën lo·∫°i b·ªè\n    exclude_extensions = {'.pyc', '.pyo', '.pyd'}\n\n    result = {\n        \"project_id\": 1,\n        \"repo_name\": repo_name,\n        \"repo_info\": {\n            \"description\": f\"Tr√≠ch xu·∫•t c·∫•u tr√∫c t·ª´ th∆∞ m·ª•c: {repo_name}\",\n            \"path\": project_root\n        },\n        \"content\": {\n            branch_name: {}\n        }\n    }\n\n    try:\n        items = os.listdir(project_root)\n        for item in items:\n            item_path = os.path.join(project_root, item)\n\n            # B·ªè qua n·∫øu n·∫±m trong danh s√°ch exclude_dirs\n            if item in exclude_dirs:\n                continue\n\n            # X·ª≠ l√Ω TH∆Ø M·ª§C -> CATEGORY\n            if os.path.isdir(item_path):\n                category_name = item\n                files_in_category = []\n\n                for root, _, filenames in os.walk(item_path):\n                    # Ki·ªÉm tra xem folder hi·ªán t·∫°i c√≥ thu·ªôc folder b·ªã c·∫•m kh√¥ng (v√≠ d·ª• __pycache__ b√™n trong src)\n                    if any(ex in root.split(os.sep) for ex in exclude_dirs):\n                        continue\n\n                    for fname in filenames:\n                        # 3. Ki·ªÉm tra ph·∫ßn m·ªü r·ªông file\n                        if any(fname.endswith(ext) for ext in exclude_extensions):\n                            continue\n\n                        rel_path = os.path.relpath(os.path.join(root, fname), item_path)\n                        files_in_category.append(rel_path.replace(\"\\\\\", \"/\"))\n\n                if files_in_category:\n                    result[\"content\"][branch_name][category_name] = sorted(files_in_category)\n\n            # X·ª≠ l√Ω FILE ·ªü g·ªëc\n            elif os.path.isfile(item_path):\n                # Ki·ªÉm tra ph·∫ßn m·ªü r·ªông cho file ·ªü g·ªëc\n                if any(item.endswith(ext) for ext in exclude_extensions):\n                    continue\n\n                if \"root_files\" not in result[\"content\"][branch_name]:\n                    result[\"content\"][branch_name][\"root_files\"] = []\n                result[\"content\"][branch_name][\"root_files\"].append(item)\n\n    except Exception as e:\n        print(f\"L·ªói khi duy·ªát th∆∞ m·ª•c: {e}\")\n\n    return [result]\n\n\ndef save_matched_json(project_path, output_file):\n    repo_name = os.path.basename(os.path.normpath(project_path))\n    final_data = extract_project_to_match_format(project_path, repo_name=repo_name)\n\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(final_data, f, ensure_ascii=False, indent=2)\n\n    print(f\"--- ƒê√£ ho√†n th√†nh ---\")\n    print(f\"Project: {repo_name}\")\n    print(f\"ƒê√£ lo·∫°i b·ªè: .pyc, log/, __pycache__/\")\n    print(f\"K·∫øt qu·∫£ l∆∞u t·∫°i: {output_file}\")\n\n\nif __name__ == \"__main__\":\n    # T√¨m project root (GraphRAG)\n    current_file = os.path.abspath(__file__)\n    root = os.path.dirname(os.path.dirname(os.path.dirname(current_file)))\n    save_matched_json(root, \"../../data/extracted_sample_format.json\")"
    },
    {
        "file_name": "repo_struct.py",
        "file_path": "src\\test\\repo_struct.py",
        "imports": [
            "from langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_community.vectorstores import Neo4jVector\nimport json\nimport src.connection as connection\nfrom src.prompt.index.community_report_new import BATCH_COMMUNITY_REPORT_PROMPT\nfrom src.prompt.index.extract_graph import GRAPH_EXTRACTION_PROMPT\nfrom src.prompt.index.extract_graph_code_repo import GRAPH_EXTRACTION_REPO_PROMPT\nfrom src.prompt.index.community_report import COMMUNITY_REPORT_PROMPT\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom neo4j import GraphDatabase\n\nfrom langchain_core.output_parsers import StrOutputParser\nimport networkx as nx\nimport os\n\n\ndef run_ingestion_for_repo_struct(repo_structure_data, import_analysis_data)",
            "import time\n    t1"
        ],
        "content": "from langchain_core.prompts import PromptTemplate\nfrom langchain_core.output_parsers import JsonOutputParser\nfrom langchain_community.vectorstores import Neo4jVector\nimport json\nimport src.connection as connection\nfrom src.prompt.index.community_report_new import BATCH_COMMUNITY_REPORT_PROMPT\nfrom src.prompt.index.extract_graph import GRAPH_EXTRACTION_PROMPT\nfrom src.prompt.index.extract_graph_code_repo import GRAPH_EXTRACTION_REPO_PROMPT\nfrom src.prompt.index.community_report import COMMUNITY_REPORT_PROMPT\nfrom langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\nfrom neo4j import GraphDatabase\n\nfrom langchain_core.output_parsers import StrOutputParser\nimport networkx as nx\nimport os\n\n\ndef run_ingestion_for_repo_struct(repo_structure_data, import_analysis_data):  # ƒê·ªïi t√™n tham s·ªë cho ƒë√∫ng b·∫£n ch·∫•t JSON\n    import time\n    t1 = time.time()\n    print(\"[1/3] Running Extraction...\")\n\n    # C·∫•u h√¨nh c·ª• th·ªÉ d·∫•u ph√¢n c√°ch\n    T_DELIM = \"|\"\n    R_DELIM = \"##\"  # D√πng k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ªÉ tr√°nh l·∫´n v·ªõi d·∫•u xu·ªëng d√≤ng trong description\n    C_DELIM = \"<DONE>\"\n\n    prompt = PromptTemplate.from_template(GRAPH_EXTRACTION_REPO_PROMPT)\n    chain = prompt | connection.llm | StrOutputParser()\n\n    try:\n        result_text = chain.invoke({\n            \"import_analysis\": json.dumps(import_analysis_data, indent=2, ensure_ascii=False),\n            \"repo_structure\": json.dumps(repo_structure_data, indent=2, ensure_ascii=False),\n            \"entity_types\": \"PROJECT, BRANCH, CATEGORY, FILE\",\n            \"tuple_delimiter\": T_DELIM,\n            \"record_delimiter\": R_DELIM,\n            \"completion_delimiter\": C_DELIM\n        })\n\n        # L∆∞u log ƒë·ªÉ debug\n        os.makedirs(\"log/index\", exist_ok=True)\n        with open(\"log/index/resultindex.txt\", \"w\", encoding=\"utf-8\") as f:\n            f.write(result_text)\n\n    except Exception as e:\n        print(f\"Extraction Failed: {e}\")\n        return\n\n    entities = []\n    relationships = []\n\n    # T√°ch theo Record Delimiter ƒë√£ ƒë·ªãnh nghƒ©a\n    records = result_text.strip().split(R_DELIM)\n\n    for record in records:\n        record = record.strip()\n        if not record or C_DELIM in record: continue\n\n        # X·ª≠ l√Ω d·ªçn d·∫πp d·∫•u ngo·∫∑c k√©p ho·∫∑c ngo·∫∑c ƒë∆°n th·ª´a n·∫øu LLM t·ª± √Ω th√™m v√†o\n        record = record.strip('() \"')\n\n        parts = record.split(T_DELIM)\n\n        # Parse Entity: \"entity\"|name|type|desc\n        if \"entity\" in parts[0].lower() and len(parts) >= 4:\n            entities.append({\n                \"name\": parts[1].strip(),\n                \"type\": parts[2].strip(),\n                \"desc\": parts[3].strip()\n            })\n\n        # Parse Relationship: \"relationship\"|src|tgt|desc|strength\n        elif \"relationship\" in parts[0].lower() and len(parts) >= 5:\n            # √âp ki·ªÉu strength v·ªÅ s·ªë, m·∫∑c ƒë·ªãnh l√† 1 n·∫øu l·ªói\n            try:\n                strength = int(parts[4].strip())\n            except:\n                strength = 1\n\n            relationships.append({\n                \"source\": parts[1].strip(),\n                \"target\": parts[2].strip(),\n                \"desc\": parts[3].strip(),\n                \"strength\": strength\n            })\n\n    print(f\"   -> Extracted {len(entities)} Entities & {len(relationships)} Relationships.\")\n\n    # L∆∞u JSON h·ª£p l·ªá\n    with open(\"log/index/EntityRelationship.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump({\"entities\": entities, \"relationships\": relationships}, f, ensure_ascii=False, indent=2)\n\n    # N·∫°p v√†o Neo4j\n    print(\"   -> Writing to Neo4j...\")\n\n    for ent in entities:\n        # L√†m s·∫°ch label: X√≥a kho·∫£ng tr·∫Øng, k√Ω t·ª± ƒë·∫∑c bi·ªát ƒë·ªÉ l√†m Label Neo4j\n        label = ent['type'].upper().replace(\" \", \"_\").strip()\n\n        query = f\"\"\"\n            MERGE (e:Entity {{id: $name}})\n            SET e:{label}, e.type = $type, e.desc = $desc\n        \"\"\"\n        connection.graph.query(query, {\n            \"name\": ent['name'],\n            \"type\": ent['type'],\n            \"desc\": ent['desc']\n        })\n\n    for rel in relationships:\n        connection.graph.query(\n            \"\"\"\n            MATCH (a:Entity {id: $src}), (b:Entity {id: $tgt})\n            MERGE (a)-[r:CONNECTED_TO]->(b)\n            SET r.desc = $desc, \n                r.strength = $strength\n            \"\"\",\n            {\n                \"src\": rel['source'],\n                \"tgt\": rel['target'],\n                \"desc\": rel['desc'],\n                \"strength\": rel['strength']\n            }\n        )\n\n    t2 = time.time()\n    print(f\"Ho√†n th√†nh! T·ªïng th·ªùi gian: {round(t2 - t1, 2)} (s)\")"
    }
]