import streamlit as st
import time
import os
import sys

# ƒê·∫£m b·∫£o python t√¨m th·∫•y c√°c module trong th∆∞ m·ª•c src
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import c√°c module c·ªßa b·∫°n
import src.connection as connection
from src.graph import (run_ingestion, run_clustering_louvain, run_summarization)
from src.retrieval import router_search, global_search, \
    local_search  # L∆∞u √Ω: route_question hay router_search tu·ª≥ t√™n h√†m b·∫°n ƒë·∫∑t

st.set_page_config(
    page_title="Network GraphRAG AI",
    page_icon="üï∏Ô∏è",
    layout="wide"
)

# CSS T√ôY CH·ªàNH
st.markdown("""
<style>
    /* Ch·ªânh m√†u n·ªÅn v√† border cho khung chat */
    .stChatMessage {
        border-radius: 15px;
        padding: 10px;
        margin-bottom: 10px;
        border: 1px solid #e0e0e0;
    }
    /* L√†m ƒë·∫≠m c√°c th·∫ª Header trong Markdown */
    h2, h3 {
        color: #2E86C1; /* M√†u xanh chuy√™n nghi·ªáp */
    }
    /* Hi·ªáu ·ª©ng cho status box */
    .stStatusWidget {
        border-radius: 10px;
    }
</style>
""", unsafe_allow_html=True)


#H√ÄM X·ª¨ L√ù FILE UPLOAD
def process_uploaded_yaml(uploaded_file):
    """
    H√†m ƒë·ªçc file upload t·ª´ Streamlit v·ªõi c∆° ch·∫ø b·∫Øt l·ªói an to√†n.
    """
    if uploaded_file is None:
        st.warning("Vui l√≤ng upload file YAML tr∆∞·ªõc khi x√¢y d·ª±ng Graph.")
        return None

    try:
        content = uploaded_file.read().decode("utf-8")
        if not content.strip():
            st.error("L·ªói: File t·∫£i l√™n b·ªã r·ªóng!")
            return None
        return content
    except Exception as e:
        st.error(f"L·ªói khi ƒë·ªçc file: {e}")
        return None


# --- INIT CONNECTION (Ch·ªâ ch·∫°y 1 l·∫ßn) ---
@st.cache_resource
def setup_connections():
    try:
        connection.init_connections()
        return True
    except Exception as e:
        st.error(f"L·ªói k·∫øt n·ªëi Neo4j/Gemini: {e}")
        return False


if not setup_connections():
    st.stop()

# ==========================================
# --- SIDEBAR (QUAN TR·ªåNG: KH√îNG ƒê∆Ø·ª¢C THI·∫æU) ---
# ==========================================
with st.sidebar:
    st.title(" Admin Control")
    st.markdown("---")

    st.subheader("1. Qu·∫£n l√Ω D·ªØ li·ªáu Graph")
    uploaded_file = st.file_uploader("Upload file YAML c·∫•u h√¨nh m·∫°ng", type=["yml", "yaml"])

    if st.button("X√¢y d·ª±ng Graph (Full Flow)", type="primary"):
        # G·ªçi h√†m x·ª≠ l√Ω file an to√†n
        yaml_content = process_uploaded_yaml(uploaded_file)

        # Ch·ªâ ch·∫°y ti·∫øp n·∫øu c√≥ n·ªôi dung
        if yaml_content:
            with st.status("ƒêang x√¢y d·ª±ng Knowledge Graph...", expanded=True) as status:
                st.write("1. Reading & Ingesting Data...")
                run_ingestion(yaml_content)

                st.write("2. Running Louvain Clustering...")
                run_clustering_louvain()

                status.update(label="X√¢y d·ª±ng Graph ho√†n t·∫•t!", state="complete", expanded=False)
            st.success("H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")

    st.markdown("---")
    st.subheader("2. Ch·∫ø ƒë·ªô T√¨m ki·∫øm")
    # ƒê√ÇY L√Ä CH·ªñ KHAI B√ÅO BI·∫æN search_mode
    search_mode = st.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô:",
        ("Auto (AI Router)", "Global Search (T·ªïng quan)", "Local Search (Chi ti·∫øt)")
    )

    st.markdown("---")
    if st.button("X√≥a l·ªãch s·ª≠ chat"):
        st.session_state.messages = []
        st.rerun()


# MAIN CHAT INTERFACE
st.title("üï∏Ô∏è Network GraphRAG Assistant")
st.caption("Powered by Neo4j & Gemini 1.5 Flash | Graph-based Reasoning")

# 1. Kh·ªüi t·∫°o l·ªãch s·ª≠ chat
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant",
         "content": "Xin ch√†o! T√¥i l√† tr·ª£ l√Ω m·∫°ng AI. T√¥i ƒë√£ s·∫µn s√†ng ph√¢n t√≠ch h·ªá th·ªëng c·ªßa b·∫°n."}
    ]

# 2. Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
for msg in st.session_state.messages:
    avatar = "ü§ñ" if msg["role"] == "assistant" else "üßë‚Äçüíª"
    with st.chat_message(msg["role"], avatar=avatar):
        st.markdown(msg["content"])

# 3. X·ª≠ l√Ω Input
if prompt := st.chat_input("VD: H·ªá th·ªëng c√≥ ƒëi·ªÉm ƒë∆°n th·∫•t b·∫°i (SPOF) n√†o kh√¥ng?"):
    # Hi·ªÉn th·ªã c√¢u h·ªèi User
    with st.chat_message("user", avatar="üßë‚Äçüíª"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # X·ª≠ l√Ω c√¢u tr·∫£ l·ªùi AI
    with st.chat_message("assistant", avatar="ü§ñ"):
        message_placeholder = st.empty()
        full_response = ""

        # D√πng st.status thay cho spinner
        with st.status("ƒêang ph√¢n t√≠ch h·ªá th·ªëng...", expanded=True) as status:
            try:
                # Logic ch·ªçn h√†m search (S·ª≠ d·ª•ng bi·∫øn search_mode t·ª´ sidebar)
                if search_mode == "Auto (AI Router)":
                    st.write("Targeting: AI Router Decision...")
                    # L∆∞u √Ω: Import ƒë√∫ng t√™n h√†m router c·ªßa b·∫°n (route_question ho·∫∑c router_search)
                    response_text = router_search(prompt)
                elif search_mode == "Global Search (T·ªïng quan)":
                    st.write("Targeting: Global Map-Reduce Analysis...")
                    response_text = global_search(prompt)
                else:
                    st.write("Targeting: Local Entity Traversal...")
                    response_text = local_search(prompt)

                status.update(label="Ph√¢n t√≠ch ho√†n t·∫•t!", state="complete", expanded=False)

                # Hi·ªáu ·ª©ng g√µ ch·ªØ
                for chunk in response_text.split(" "):
                    full_response += chunk + " "
                    time.sleep(0.01)
                    message_placeholder.markdown(full_response + "‚ñå")

                message_placeholder.markdown(full_response)

            except Exception as e:
                status.update(label="Err: C√≥ l·ªói x·∫£y ra!", state="error")
                st.error(f"Chi ti·∫øt l·ªói: {e}")
                full_response = "Xin l·ªói, t√¥i g·∫∑p s·ª± c·ªë khi truy xu·∫•t d·ªØ li·ªáu."
                message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})